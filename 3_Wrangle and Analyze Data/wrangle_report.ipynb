{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeRateDogs - Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed the instructions given by Udacity on how to gather the data for this data wrangling excercise.\n",
    "\n",
    "- I downloaded the WeRateDogs™ Twitter Archive as a .csv (twitter_archive_enhanced.csv) from the Udacity **3. Project Details**\n",
    "- Afterwards I programmatically downloaded the Tweet image predictions (image_predictions.tsv), as advised.\n",
    "- Because I didn't create a twitter developer account I just downloaded the additional twitter data from Udacity(tweet-json.txt).\n",
    "\n",
    "Once I gathered the different data sources I created a DataFrame for each of them.\n",
    "\n",
    "- *df_twitter* - This is the dataset \"twitter-archive-enhanced.csv\" and gives information on basic tweet data.  \n",
    "\n",
    "- *df_predict* - This is the dataset \"image_predictions.tsv\" and contains image predictions per tweet.\n",
    "\n",
    "- *df_add* - This is the dataset \"tweet-json.txt\" and contains additional info about the tweets. Here I preprocessed the id-column, to match with the other DataFrames (renamed \"id\" to \"tweet_id\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Assessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I created an overview of the various columns and their descriptions in this dataset. For every DataFrame I looked at the info(), got a glimpse of the DataFrame by looking at the .head(), .tail() or .sample() and tried to understand it that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. `Enhanced Twitter Archive`\n",
    "\n",
    "The WeRateDogs™ Twitter archive contains basic tweet data for 5000+ of their tweets, but it does not include every tweet. \n",
    "There is a column though which contains the text for each tweet, which we used to extract the rating, dog name, and dog \"type\" (doggo, floofer, pupper, and puppo).\n",
    "During the assessment we found out that:\n",
    "- not all tweets could be classified correctly\n",
    "- HTML code in the source column\n",
    "- records/tweets where the name is \"None\" or the names are \"not real\" (like a, by etc.)\n",
    "- incorrect datatypes for tinmetamp columns -> should be datetime instead of objects\n",
    "\n",
    "\n",
    "`df_twitter` columns and their description:\n",
    "    \n",
    "- **tweet_id**: the unique identifier for each of tweet\n",
    "- **in_reply_to_status_id**: the status id for the reply given to the tweet id\n",
    "- **in_reply_to_user_id**: the status id for the reply given to the tweet id\n",
    "- **timestamp**: timestamp of the tweet\n",
    "- **source**: source of the tweet\n",
    "- **text**: the corresponding tweets text\n",
    "- **retweeted_status_id**: the status id for the reply given to the tweet id\n",
    "- **retweeted_status_user_id**: the status id for the reply given to the tweet id\n",
    "- **retweeted_status_timestamp**: timestamp of the retweet\n",
    "- **expanded_urls**: Full URL for the tweet.\n",
    "- **rating_numerator**: the numerator of the given rank (5 if the rating is 5/10)\n",
    "- **rating_denominator**: the denominator of the given rank (10 if the rating is 5/10)\n",
    "- **name**: name of the dog\n",
    "- **doggo**, **floofer**,  **pupper**, **puppo** -- The stage of the dog type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Quality - df_twitter`\n",
    "\n",
    "1. Missing values in columns from in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id , retweeted_status_user_id,\n",
    "  retweeted_status_timestamp, expanded_urls\n",
    "\n",
    "2. Incorrect data types for timestamp columns\n",
    "\n",
    "3. Some names are not real names, like \"None\", \"a\", \"the\" etc. - there are more! Basically those are normal words and not names. \n",
    "\n",
    "4. Dogs could only be correctly classified 16.13% of the time (380/(1976+380))\n",
    "\n",
    "5. There are a wide range of rating_numerators in this dataset, going as high as 1776! The scale only goes from one to ten though...\n",
    "\n",
    "6. Checking some of the entries we can see that those are joke tweets (a chicken for example)\n",
    "\n",
    "7. Inconsistent missing values in the doggo, floofer etc. columns (sometimes \"None\" sometimes real NULL values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `Quality - df_predict :`\n",
    "\n",
    "WeRateDogs Twitter archive was ran through a neural network that can classify breeds of dogs. The result is a table full of image predictions (the top three only) alongside each tweet_ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "`df_predict` columns and their description:\n",
    "    \n",
    "- **tweet_id**: tweet_id is the last part of the tweet URL after \"status/\"\n",
    "- **jpg_url**: image link or URL\n",
    "- **img_num**: image number\n",
    "- **p1**: p1 is the algorithm's #1 prediction for the image in the tweet \n",
    "- **p1_conf**: p1_conf is how confident the algorithm is in its #1 prediction\n",
    "- **p1_dog**: p1_dog is whether or not the #1 prediction is a breed of dog\n",
    "- **p2**: is the algorithm's second most likely prediction\n",
    "- **p2_conf**: is how confident the algorithm is in its #2 prediction\n",
    "- **p2_dog**:  is whether or not the #2 prediction is a breed of dog \n",
    "- **p3**: p3 is the algorithm's #3 prediction for the image in the tweet\n",
    "- **p3_conf**: p3_conf is how confident the algorithm is in its #3 prediction\n",
    "- **p3_dog**: p3_dog is whether or not the #3 prediction is a breed of dog\n",
    "\n",
    "\n",
    "`Quality - df_predict` table:\n",
    "\n",
    "- No NULL values\n",
    "\n",
    "- But we can clearly see that there is no standardized format for the dog breed/race prediction as they sometimes start with capital letters and sometimes they do not (German_shepherd but miniature_pinscher)\n",
    "\n",
    "- Additionally they are separated by and underscore instead of a whitespace\n",
    "\n",
    "- Lastly there are pictures, which have no prediction at all (p1_dog, p2_dog and p3_dog are all False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `df_add`\n",
    "\n",
    "`df_add` columns and their description:\n",
    "\n",
    "- **tweet_id**: the unique identifier for each of tweet\n",
    "- **retweets**: the count of retweets\n",
    "- **favorites**: The count of favourites\n",
    "- **followers**: The count of followers\n",
    "- **friends**: The count of friends\n",
    "\n",
    "#### `Quality - df_add` table\n",
    "\n",
    "- No NULL values again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Cleaning\n",
    "\n",
    "Cleaning steps I did everything is more detailed in the wrangle_act.ipynb:\n",
    "\n",
    "\n",
    "**1. Merge tables - TIDINESS ISSUE!** \n",
    "\n",
    "**2. Drop everything we found before. Retweets, replies, tweets without an image or tweets without dogs - QUALITY ISSUE!**\n",
    "\n",
    "**3. Fixing datatypes  - QUALITY ISSUE!**\n",
    "\n",
    "**4. Clean the source column - QUALITY ISSUE!**\n",
    "\n",
    "**5. Split the text range into two separate columns - TIDINESS ISSUE!**\n",
    "\n",
    "**6. Merging the different classification columns (doggo, floofer etc.) into one column and removing \"None\" entries - TIDINESS ISSUE!**\n",
    "\n",
    "**7. Remove incorrect names - QUALITY ISSUE!**\n",
    "\n",
    "**8. Just use the column with the highest % and the respective prediction - TIDINESS ISSUE!**\n",
    "\n",
    "**9. Standardize the breeds, because some are uppercase - QUALITY ISSUE!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Store\n",
    "\n",
    "I stored the final dataframe into a .csv file with name **twitter_archive_master.csv** (as instructed by Udacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}